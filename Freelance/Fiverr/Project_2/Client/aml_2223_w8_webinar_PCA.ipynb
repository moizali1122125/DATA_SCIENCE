{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## AML 22-23 S2W8 Webinar PCA"
      ],
      "metadata": {
        "id": "0LvNEaOaIzy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Points"
      ],
      "metadata": {
        "id": "m9MHYcBSJHmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Why do we do it? What is the purpose?\n",
        "    - **dimensionality reduction**: working with a smaller set of features; this could benefit performance (train/prediction time), possibly explainability too (smaller set of factors/predictors); some feature might hurt model performance; pretty much the same as for **feature selection**.\n",
        "\n",
        "    - **dimensionality reduction**: transformation of the original data space; blending of features.\n",
        "\n",
        "* No free lunch. You do throw away information, and you would lose on explainability - how do you explain the new data space to stakeholders? Trade-offs...\n",
        "\n",
        "    - You can have many models! For example, you have a PCA-based model that makes better predictions at diagnosing malignant tumors. You could use that as \"triage\". Then, a second model could be picked, zooming into the maligant tumors, that is more explainable and potentially more suitable for communication with stakeholders.\n",
        "\n",
        "* Practicalities:\n",
        "    - It needs to be mean-centered so that the underlying SVD or eigen-based solution can work correctly on the covariance matrix. A `StandardScaler()` would do that job (the feature ends up with mean 0 and std 1).\n",
        "\n",
        "* Q/A:\n",
        "    - PCA is unsupervised (it knows nothing about the target, assuming that we using a PCA-transformed dataset in a supervised, downstream task (e.g., classification/regression)). Isn't there a danger of PCA not paying much attention to a good predictor (that is, something that would have been high correlated to the target)?\n",
        "        + Yes. There could in principle a feature that is potentially a good predictor but with, say, low variance, that is not favoured by PCA.\n",
        "        + Further reading: https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html"
      ],
      "metadata": {
        "id": "cP81iAUhJJuF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ozVp7G7ZJPJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}