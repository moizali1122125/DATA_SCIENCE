{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # **Best Model Selection**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Parameters: {'model__max_depth': 20, 'model__min_samples_leaf': 1, 'model__min_samples_split': 5, 'model__n_estimators': 10}\n",
      "Random Forest Test Accuracy: 0.8156424581005587\n",
      "Random Forest Test Precision: 0.8253968253968254\n",
      "Random Forest Test Recall: 0.7027027027027027\n",
      "Random Forest Test F1 Score: 0.7591240875912408\n",
      "Gradient Boosting Best Parameters: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 50}\n",
      "Gradient Boosting Test Accuracy: 0.7988826815642458\n",
      "Gradient Boosting Test Precision: 0.8166666666666667\n",
      "Gradient Boosting Test Recall: 0.6621621621621622\n",
      "Gradient Boosting Test F1 Score: 0.7313432835820896\n",
      "Best Model: RandomForestClassifier(random_state=42)\n",
      "Best Accuracy: 0.8156424581005587\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Titanic dataset from Seaborn\n",
    "titanic_data = sns.load_dataset('titanic')\n",
    "\n",
    "# Select features and target variable\n",
    "X = titanic_data[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = titanic_data['survived']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a list of models to evaluate\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Iterate over the models and evaluate their performance\n",
    "for name, model in models:\n",
    "    # Create a pipeline for each model\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Create a parameter grid for the model\n",
    "    if name == 'Random Forest':\n",
    "        param_grid = {\n",
    "            'model__n_estimators': [10, 50, 100, 200],\n",
    "            'model__max_depth': [None, 5, 10, 20],\n",
    "            'model__min_samples_split': [2, 5, 10],\n",
    "            'model__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    elif name == 'Gradient Boosting':\n",
    "        param_grid = {\n",
    "            'model__n_estimators': [10, 50, 100],\n",
    "            'model__learning_rate': [0.1, 0.01],\n",
    "            'model__max_depth': [3, 5],\n",
    "        }\n",
    "\n",
    "    # Create a grid search object\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid)\n",
    "\n",
    "    # Fit the grid search on the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters for the model\n",
    "    print(f'{name} Best Parameters: {grid_search.best_params_}')\n",
    "\n",
    "    # Make predictions on the test data using the best model\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Print the evaluation metrics for the model\n",
    "    print(f'{name} Test Accuracy: {accuracy}')\n",
    "    print(f'{name} Test Precision: {precision}')\n",
    "    print(f'{name} Test Recall: {recall}')\n",
    "    print(f'{name} Test F1 Score: {f1}')\n",
    "\n",
    "    # Update the best model and accuracy if necessary\n",
    "    if accuracy > best_accuracy:\n",
    "        best_model = model\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "print(f'Best Model: {best_model}')\n",
    "print(f'Best Accuracy: {best_accuracy}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Parameters: {'model__max_depth': 5, 'model__min_samples_leaf': 2, 'model__min_samples_split': 10, 'model__n_estimators': 100}\n",
      "Random Forest Test MAE: 0.2701324653948732\n",
      "Random Forest Test MSE: 0.14849249573022463\n",
      "Random Forest Test RMSE: 0.38534724046011365\n",
      "Random Forest Test R2 Score: 0.3876643428967661\n",
      "Gradient Boosting Best Parameters: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 100}\n",
      "Gradient Boosting Test MAE: 0.26911976508719937\n",
      "Gradient Boosting Test MSE: 0.13533876632460917\n",
      "Gradient Boosting Test RMSE: 0.3678841751483871\n",
      "Gradient Boosting Test R2 Score: 0.44190612460658907\n",
      "Best Model: GradientBoostingRegressor(random_state=42)\n",
      "Best R2 Score: 0.44190612460658907\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the Titanic dataset from Seaborn\n",
    "titanic_data = sns.load_dataset('titanic')\n",
    "\n",
    "# Select features and target variable\n",
    "X = titanic_data[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = titanic_data['survived']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a list of models to evaluate\n",
    "models = [\n",
    "    ('Random Forest', RandomForestRegressor(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingRegressor(random_state=42))\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_r2_score = 0.0\n",
    "\n",
    "# Iterate over the models and evaluate their performance\n",
    "for name, model in models:\n",
    "    # Create a pipeline for each model\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model', model)\n",
    "    ]) \n",
    "    # Create a parameter grid for the model\n",
    "    if name == 'Random Forest':\n",
    "        param_grid = {\n",
    "            'model__n_estimators': [10, 50, 100, 200],\n",
    "            'model__max_depth': [None, 5, 10, 20],\n",
    "            'model__min_samples_split': [2, 5, 10],\n",
    "            'model__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    elif name == 'Gradient Boosting':\n",
    "        param_grid = {\n",
    "            'model__n_estimators': [10, 50, 100],\n",
    "            'model__learning_rate': [0.1, 0.01],\n",
    "            'model__max_depth': [3, 5],\n",
    "        } \n",
    "    # Create a grid search object\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid) \n",
    "    # Fit the grid search on the training data\n",
    "    grid_search.fit(X_train, y_train) \n",
    "    # Print the best parameters for the model\n",
    "    print(f'{name} Best Parameters: {grid_search.best_params_}') \n",
    "    # Make predictions on the test data using the best model\n",
    "    y_pred = grid_search.predict(X_test) \n",
    "    # Calculate evaluation metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test,y_pred,squared=False)\n",
    "    r2 = r2_score(y_test,y_pred) \n",
    "    # Print the evaluation metrics for the model\n",
    "    print(f'{name} Test MAE: {mae}')\n",
    "    print(f'{name} Test MSE: {mse}')\n",
    "    print(f'{name} Test RMSE: {rmse}')\n",
    "    print(f'{name} Test R2 Score: {r2}') \n",
    "    # Update the best model and r2 score if necessary\n",
    "    if r2 > best_r2_score:\n",
    "        best_model = model\n",
    "        best_r2_score = r2 \n",
    "print(f'Best Model: {best_model}')\n",
    "print(f'Best R2 Score: {best_r2_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure(figsize=(12,4))\\nfor i in ['mean_test_precision','mean_test_recall', 'mean_test_minimum_of_both']:\\n    plt.plot([j[1]for j in df_results['param_class_weight']],\\n    df_results[i],\\n    label=i)\\nplt.legend() \\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make this plot after Best Model selection Chat gpt\n",
    "'''\n",
    "plt.figure(figsize=(12,4))\n",
    "for i in ['mean_test_precision','mean_test_recall', 'mean_test_minimum_of_both']:\n",
    "    plt.plot([j[1]for j in df_results['param_class_weight']],\n",
    "    df_results[i],\n",
    "    label=i)\n",
    "plt.legend() \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
