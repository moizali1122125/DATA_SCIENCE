{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # **Hyperparameter Tunning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few common methods to find the optimal hyperparameters for a model:\n",
    "\n",
    "1. **Manual Tuning**: In manual tuning, you adjust the hyperparameters based on your knowledge of the problem and the model, and evaluate the model performance on a validation set. This method can be useful if you have limited computational resources or if the hyperparameters have a clear interpretation.\n",
    "\n",
    "2. **Grid Search**: In grid search, you define a range of possible values for each hyperparameter, and the grid search algorithm exhaustively evaluates all possible combinations of hyperparameters using cross-validation to find the best combination. This method can be computationally expensive, especially if there are many hyperparameters and possible values.\n",
    "\n",
    "3. **Random Search**: In random search, you define a range of possible values for each hyperparameter, and the algorithm randomly samples hyperparameter combinations from this space. This method can be faster than grid search, especially if there are many hyperparameters and possible values.\n",
    "\n",
    "4. **Bayesian Optimization**: In Bayesian optimization, you define a prior probability distribution over the hyperparameters, and the algorithm updates this distribution based on the results of previous evaluations to suggest the next hyperparameter combination to evaluate. This method can be more efficient than grid search and random search for complex models with many hyperparameters.\n",
    "\n",
    "It's important to note that the best method depends on the specific model and problem, and a combination of these methods may be used to find the optimal hyperparameters.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1- Grid Search CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC # support vector Classifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load the iris dataset and create a pipeline with an SVM classifier\n",
    "iris = load_iris()\n",
    "pipeline = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "# Define the hyperparameters to test using grid search\n",
    "param_grid = {\n",
    "    'svc__C': [0.1, 1, 10],\n",
    "    'svc__kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid = GridSearchCV(pipeline, param_grid = param_grid, cv=5)\n",
    "grid.fit(iris.data, iris.target)\n",
    "\n",
    "# Print the best hyperparameters and the mean cross-validation score\n",
    "print(\"Best hyperparameters: \", grid.best_params_)\n",
    "print (\"Cross-validation score: \", grid.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2- Random Search CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC # support vector Classifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# Load the iris dataset and create a pipeline with an SVM classifier\n",
    "iris = load_iris()\n",
    "pipeline = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "# Define the hyperparameters to test using random search\n",
    "param_dist = {\n",
    "'svc__C': sp_randint(1, 100),\n",
    "'svc__kernel': ['linear', 'rbf', 'poly'],\n",
    "}\n",
    "\n",
    "# Perform random search with 5-fold cross-validation\n",
    "grid = RandomizedSearchCV(pipeline, param_distributions=param_dist, cv=5, n_iter=10)\n",
    "grid.fit(iris.data, iris.target)\n",
    "\n",
    "# Print the best hyperparameters and the mean cross-validation score\n",
    "print(\"Best hyperparameters: \", grid.best_params_)\n",
    "print (\"Cross-validation score: \", grid.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3- Bayesian Optimization CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical\n",
    "\n",
    "# Load the iris dataset and create a pipeline with an SVM classifier\n",
    "iris = load_iris()\n",
    "pipeline = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "# Define the hyperparameters to test using Bayesian optimization\n",
    "param_dist = {\n",
    "    'svc__C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "    'svc__kernel': Categorical(['linear', 'rbf', 'poly'])\n",
    "}\n",
    "\n",
    "# Perform Bayesian optimization with 5-fold cross-validation\n",
    "grid = BayesSearchCV(pipeline, param_distributions=param_dist, cv=5, n_iter=10)\n",
    "grid.fit(iris.data, iris.target)\n",
    "\n",
    "# Print the best hyperparameters and the mean cross-validation score\n",
    "print(\"Best hyperparameters: \", grid.best_params_)\n",
    "print (\"Cross-validation score: \", grid.best_score_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4- Manual Tunning CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load the iris dataset and create a pipeline with an SVM classifier\n",
    "iris = load_iris()\n",
    "pipeline = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "# Manually adjust the hyperparameters and evaluate the model on a validation set\n",
    "pipeline.set_params(svc__C=1.8, svc__kernel='rbf')\n",
    "pipeline.fit(iris.data[:100], iris.target[:100])\n",
    "score = pipeline.score(iris.data[100:], iris.target[100:])\n",
    "print(\"Validation score: \", score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
