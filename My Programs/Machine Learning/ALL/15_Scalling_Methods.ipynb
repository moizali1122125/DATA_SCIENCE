{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # **Scalling in Machine learning**\n",
    "Data scaling is a common preprocessing step in machine learning that involves transforming the input variables to have\n",
    "Similar scale or distribution. This can help improve the performance and stability of some machine learning\n",
    "algorithms, particularly those that are sensitive to the scale of the input data, such as K-nearest neighbors.\n",
    "Support vector machines, and gradient descent-based optimization algorithms.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1_ Standard Scalar**\n",
    "Standardization is a technique that is often applied to make the mean of the data zero and the standard deviation one.\n",
    "This is done by subtracting the mean and dividing by the standard deviation. This is done for each feature in the\n",
    "dataset. The standardization is done using the StandardScaler class in the sklearn.preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  0.1  -  7.7\n",
      "X_train_scaled(StandarScalar):  -2.373777512810883  -  2.9923757343597126\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "#Define the predictor and response variables\n",
    "X = data.data\n",
    "y = data.target\n",
    "#Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2, random_state=42)\n",
    "#Perform feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train: \", X_train.min(), \" - \", X_train.max())\n",
    "print(\"X_train_scaled (StandarScalar): \", X_train_scaled.min(), \" - \", X_train_scaled.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2_ Min-Max Scalar**\n",
    "The min-max scaler is a technique that is often used to scale the data to a fixed range, such as 0 to 1. This is done by\n",
    "subtracting the minimum value and dividing by the range. This is done for each feature in the dataset. The min-max\n",
    "scaling is done using the MinMax5caler class in the sklearn.preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  0.1  -  7.7\n",
      "X_train_scaled(StandarScalar):  0.0  -  1.0\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "#Define the predictor and response variables\n",
    "X = data.data\n",
    "y = data.target\n",
    "#Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2, random_state=42)\n",
    "#Perform feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train: \", X_train.min(), \" - \", X_train.max())\n",
    "print(\"X_train_scaled (Min-Max Scaler): \", X_train_scaled.min(), \" - \", X_train_scaled.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3_ Robust Scalar**\n",
    "The RobustScaler method scales the data based on the median and interquartile range (IQR) of each feature. This can\n",
    "difference between the 75th and 25th percentiles. The IQR is calculated using the RobustScaler class in the\n",
    "sklearn.preprocessing module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  0.1  -  7.7\n",
      "X_train_scaled (RobustScaler):  -1.6666666666666665  -  2.3333333333333335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train: \", X_train.min(), \" - \", X_train.max())\n",
    "print(\"X_train_scaled (RobustScaler): \", X_train_scaled.min(), \" - \", X_train_scaled.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4_ MaxAbs Scalar**\n",
    "The MaxAbsScaler method scales the data based on the maximum absolute value of each feature. This can be useful\n",
    "when the data contains outliers or is not no rmally distributed. The maximum absolute value is calculated\n",
    "using the MaxAbsScaler class in the sklearn.preprocessing module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  0.1  -  7.7\n",
      "X_train_scaled (MaxAbsScaler):  0.04  -  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train: \", X_train.min(), \" - \", X_train.max())\n",
    "print(\"X_train_scaled (MaxAbsScaler): \", X_train_scaled.min(), \" - \", X_train_scaled.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5_ Quantile Transformer**\n",
    "The QuantileTransformer method transforms the features to folow a uniform or a normal distribution. This can be\n",
    "useful for non-linear transformations in which the output is more normally distributed. The transformation is applied\n",
    "using the QuantileTransformer class in the sklearn.preprocessing module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  0.1  -  7.7\n",
      "X_train_scaled (QuantileTransformer):  -5.199337582605575  -  5.19933758270342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOIZ\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_data.py:2623: UserWarning: n_quantiles (1000) is greater than the total number of samples (120). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "scaler = QuantileTransformer(output_distribution = 'normal')\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train: \", X_train.min(), \" - \", X_train.max())\n",
    "print(\"X_train_scaled (QuantileTransformer): \", X_train_scaled.min(), \" - \", X_train_scaled.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6_ Power transformer**\n",
    "\n",
    "The PowerTransformer method transforms the features to follow a normal distribution by applying a power\n",
    "transformation. This can be useful for non-inear transtormations in which the output is more normally distributed.\n",
    "\n",
    "The transtormation is applied using the PowerTransformer class in the sklearn.preprocessing module.\n",
    "\n",
    "The Power Transformer method applies a power transtormation to the data to make it more Gaussian-like.The method parameter can be set to yeo-johnson or boX-cox to control the type of power transtormation used.\n",
    "The default is yeo-johnson. The box-cox method is limited to strictly positive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  0.1  -  7.7\n",
      "X_train_scaled (PowerTransformer):  -2.68316691739846  -  2.6651156823636706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "scaler = PowerTransformer (method='yeo-johnson')\n",
    "X_train_scaled = scaler.fit_transform (X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train: \", X_train.min(), \" - \", X_train.max())\n",
    "print(\"X_train_scaled (PowerTransformer): \", X_train_scaled.min(), \" - \", X_train_scaled.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7_ Normalizer**\n",
    "\n",
    "The Normalizer method transforms the data to have a unit norm.\n",
    "\n",
    "This can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using algorithms that\n",
    "weignt input values Such as neural networks and a lgoritnms that use distance measures such as K-nearest neighbors.\n",
    "\n",
    "The transformation is applied using the Normalizer class in the sklearn.preprocessing module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  0.1  -  7.7\n",
      "X_train_scaled (Normalizer):  0.014726598240177802  -  0.8609385732675535\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train: \", X_train.min(), \" - \", X_train.max())\n",
    "print(\"X_train_scaled (Normalizer): \", X_train_scaled.min(), \" - \", X_train_scaled.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Normalizer method scales each sample (1.e., each row in the data matrix) to have unit norm. This can be useful\n",
    "when you want to treat each sample as a vector with a certain magnitude and direction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8_ Binarizer**\n",
    "\n",
    "The Binarizer method converts the data to binary values (1.e, 0 or 1) based on a threshold. This can be useful\n",
    "when you want to treat the data as a binary classification problem.\n",
    "\n",
    "These are just a few more examples of the scaling methods available in scikit-learn.\n",
    "\n",
    "You can find more information on scaling techniques in the scikit-learn documentation.\n",
    "\n",
    "It's important to choose the appropriate Scaling method based on the specific problem and data at hand, and to\n",
    "experiment with different techniques to find the best approach for your particular problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  0.1  -  7.7\n",
      "X_train_scaled (Binarizer):  0.0  -  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn. preprocessing import Binarizer\n",
    "scaler = Binarizer(threshold=0.5)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train: \", X_train.min(), \" - \", X_train.max())\n",
    "print(\"X_train_scaled (Binarizer): \", X_train_scaled.min(), \" - \", X_train_scaled.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
