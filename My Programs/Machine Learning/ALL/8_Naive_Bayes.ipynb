{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # **Naive Bayes Algorithms**\n",
    "- Naive Bayes is a probabilistic machine learning algorithm based on Bayes' theorem, which describes the probability of an event based on prior knowledge or evidence.\n",
    "\n",
    "- In the context of classification problems, Naive Bayes is used to predict the class label of a given sample based on its features.\n",
    "\n",
    "- The naive in Naive Bayes refers to the assumption that the features in the dataset are independent of each other, given the class label.\n",
    "\n",
    "- This is known as the independence assumption', and it simplifies the computation of the conditional probabilities that are needed to make predictions.\n",
    "\n",
    "- Naive Bayes is called a generative model, meaning that it mode ls the joint probability distribution of the features and the class label.\n",
    "\n",
    "- It can be used for both binary and multi-class classification problems, and it works well with high-dimensional datasets, making it a popular choice for text classification, spam filtering, and other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='E:/DATA_SCIENCE/My Programs/Machine Learning/Def/Types_of_Naive_Bayes_Classifier_08826a71f0.jpg', width=900)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are three main types of Naive Bayes classifiers:**\n",
    "1. Gaussian Naive Bayes: This is the most commonly used type of Naive Bayes classifier. It assumes that the continuous features in the dataset follow a 6aussian (normal) distribution.\n",
    "   \n",
    "2. Multinomial Naive Bayes: This type of Naive Bayes classifier is used for discrete data, such as text classification. It assumes that the Teatures fotlow a multinomiat distribution.\n",
    "   \n",
    "3. Bernoulli Naive Bayes: This type of Naive Bayes classifier is also used for discrete data, but it assumes that the features are binary (i.e, they take on va lues of 0 or 1). It assumes that the features follow a Bernoulli distribution\n",
    "\n",
    "Each type of Naive Bayes classifier makes different assumptions about the distribution of the features in the dataset. The choice of\n",
    "Wnich type of Naive Bayes classirier to use depends on tne nature of the data and the probiem being solved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Optimal Naive Bayes:** \n",
    "### Typically refers to an approach for selecting the best Naive Bayes algorithm and hyperparameters for a given classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split (iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#load the diamonds dataset and create a pandas dataf rame\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "# separate the features and the target variable\n",
    "X = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']]\n",
    "y = diamonds['cut']\n",
    "#split the dataset into training and testing sets\n",
    "X_train,X_test, y_train, y_test = train_test_split (X, y, test_size=0.3, random_state=42)\n",
    "# create a Gaussian Naive Bayes classifier and fit it to the training data\n",
    "gnb = GaussianNB()\n",
    "gnb.fit (X_train, y_train)\n",
    "#use the classifier to make predictions on the testing data\n",
    "y_pred = gnb.predict(X_test)\n",
    "#evaluate the accuracy of the predictions\n",
    "print (\"Accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bernoulli & Multinomial Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "#load the digits dataset and create a pandas dataframe\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "#split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# create a Bernoulli Naive Bayes classifier and fit it to the training data\n",
    "bnb = BernoulliNB(binarize=0.5)\n",
    "bnb.fit (X_train, y_train)\n",
    "#use the classifier to make predictions on the testing data\n",
    "y_pred_bnb = bnb.predict(X_test)\n",
    "# create a Multinomial Naive Bayes classifier and fit it to the train ing data\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "#use the classifier to make predictions on the testing data\n",
    "y_pred_mnb = mnb.predict(X_test)\n",
    "# evaluate the accuracy of the predictions\n",
    "print(\"Bernoulli Naive Bayes accuracy: \", accuracy_score (y_test, y_pred_bnb) )\n",
    "print(\"Multinomial Naive Bayes accuracy:\", accuracy_score (y_test, y_pred_mnb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to choose one Naive Bayes Classifier over the other?**\n",
    "When choosing a Naive Bayes algorithm to use for a given classification task, the choice between Gaussian, Multinomial, and Bernoull Naive Bayes typically depends on the type of data you are working with and the assumptions you are willing to make about the distribution of the data.\n",
    "\n",
    "Here are some general guidelines for choosing one type or Naive BayeS algorithm over tne others:\n",
    "\n",
    "- **Gaussian Naive Bayes:** This algorithm assumes that the features follow a normal (Gaussian) distribution. It works well with continuous data and can handle real-valued features. It may not work well with highly skewed or multi-model data, where other distributions may be more appropriate.\n",
    "\n",
    "\n",
    "- **Multinomial Naive Bayes:** This algorithm is used for discrete data such as word counts, which are typically non-negative integer values. It works well with text data and is often used for document classitication or spam filtering tasks.\n",
    "\n",
    "- **Bernouli Naive Bayes:** This algorithm is similar to Multinomial Naive Bayes but is specifically designed for binary data (i.e., where each feature takes on one of two values, Such as presence or absence of a feature). It is commonly used for text classification tasks such as sentiment analysis or spam filtering.\n",
    "\n",
    "In practice, it is often a good idea to try out all three altgolthms on your data and compare their perrormance using cross\n",
    "validation or other evaluation metrics. This can help you determine which algorithm is best suited for your specific task and dataset.\n",
    "Additionally, it's worth noting that in some cases, a more complex algorithm such as logistic regression or a decision tree\n",
    "may perform better than Naive Bayes, so it's always a good idea to experiment with different models to find the best one for your\n",
    "particular task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Steps to follow:**\n",
    "To implement Optimal Naive Bayes, you would typically follow these steps:\n",
    "- **Preprocess the data:** This may involve feature scaling, handling missing values, encoding categorical variables, and other steps to prepare the data for modeling.\n",
    "- **Split the data into training and testing sets:** Use a portion of the data to train the Naive Bayes model and the remainder tO test its perormance.\n",
    "- **Choose the Naive Bayes algorithm and hyperparameters:** Select the type of Naive Bayes algorithm (e.g., Gaussian, Multinomial, or Bernouli) and tune the hyperparameters (e.g, smoothing parameter, threshold) using cross-validation on the training data.\n",
    "- **Train ana evaluate the model:** Train the Naive Bayes model on the training data using the chosen algorithm and hyperparameters, and evaluate its performance on the testing data using the chosen performance metric(s).\n",
    "- **Refine the model:** Iterate on steps 3-4 to fine-tune the model, and consider using techniques Such as feature selection,dimensionality reduction, or ensemble methods to improve pertormance.\n",
    "\n",
    "Overall, the goal of Optimal Naive Bayes is to find the best Naive Bayes model for a given task by systematically evaluating different\n",
    "algorithms and hyperparameters. This can help ensure that the model performs well on new data and generalizes to unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Load the diamonds dataset and create a pandas dataframe\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "# Separate the features and the target variable\n",
    "X = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']]\n",
    "y = diamonds ['cut']\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#Preprocess the data by scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "#Define the Naive Bayes algorithms to test\n",
    "gnb = GaussianNB()\n",
    "mn = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "#Define the hyperparameters to test for each algorithm\n",
    "gnb_params = {}\n",
    "mnb_params = {'alpha': [0.1, 0.5, 1.0]}\n",
    "bnb_params = {'alpha': [0.1, 0.5, 1.0], \"binarize\": [0.8, 0.5, 1.0]}\n",
    "#Define the grid search object to f ind the best algorithm and hyperparameters\n",
    "grid = GridSearchCV(estimator=None, param_grid=None, scoring= 'accuracy', cv=5, error_score='raise')\n",
    "\n",
    "#Fit the grid search object to the training data to find the best algorithm and hype D D.\n",
    "best_score  = 0\n",
    "best_algo   = None\n",
    "best_params = None\n",
    "for algo, params in [(gnb, gnb_params),(mnb, mnb_params),(bnb, bnb_params )]:\n",
    "    grid.estimator  = algo\n",
    "    grid.param_grid = params\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    if grid.best_score_> best_score:\n",
    "        best_score  = grid.best_score_\n",
    "        best_algo   = algo\n",
    "        best_params = grid.best_params_\n",
    "# Train the best Naive Bayes model on the training data using the chosen algorithm and hyperparameters\n",
    "best_model = best_algo.set_params(**best_params)\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "#Use the best model to make predictions on the testing data\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "# Evaluate the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print (\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upper Code show error of negitive values because i use StandardScaler which convert values:  -3  to  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Load the diamonds dataset and create a pandas dataframe\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "# Separate the features and the target variable\n",
    "X = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']]\n",
    "y = diamonds ['cut']\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#Preprocess the data by scaling the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "#Define the Naive Bayes algorithms to test\n",
    "gnb = GaussianNB()\n",
    "mn = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "#Define the hyperparameters to test for each algorithm\n",
    "gnb_params = {}\n",
    "mnb_params = {'alpha': [0.1, 0.5, 1.0]}\n",
    "bnb_params = {'alpha': [0.1, 0.5, 1.0], \"binarize\": [0.8, 0.5, 1.0]}\n",
    "#Define the grid search object to f ind the best algorithm and hyperparameters\n",
    "grid = GridSearchCV(estimator=None, param_grid=None, scoring= 'accuracy', cv=5, error_score='raise')\n",
    "\n",
    "#Fit the grid search object to the training data to find the best algorithm and hype D D.\n",
    "best_score  = 0\n",
    "best_algo   = None\n",
    "best_params = None\n",
    "for algo, params in [(gnb, gnb_params),(mnb, mnb_params),(bnb, bnb_params )]:\n",
    "    grid.estimator  = algo\n",
    "    grid.param_grid = params\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    if grid.best_score_> best_score:\n",
    "        best_score  = grid.best_score_\n",
    "        best_algo   = algo\n",
    "        best_params = grid.best_params_\n",
    "# Train the best Naive Bayes model on the training data using the chosen algorithm and hyperparameters\n",
    "best_model = best_algo.set_params(**best_params)\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "#Use the best model to make predictions on the testing data\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "# Evaluate the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print ('Best Model:', best_model)\n",
    "print (\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
